# Gym Qwen: TD-MPC2 with Speculation and Decision Transformer Baselines

This repository bundles two related projects:
- **TD-MPC2 for single-task continuous control** with optional speculative execution and a learned corrector to cut replanning cost.
- The original **minimal Decision Transformer** code for D4RL-style offline RL, retained as a lightweight baseline.

## Features
- TD-MPC2 implementation adapted for fast single-task runs.
- Optional speculative execution / speculative decoding of TD-MPC2 plans.
- Learned corrector trained by distillation from the TD-MPC2 planner (two-tower and temporal variants).
- Scripts for training, collecting corrector data, distilling the corrector, and evaluating:
  - baseline TD-MPC2 (replan every step),
  - speculative TD-MPC2 + corrector with different execution horizons,
  - Decision Transformer baselines.

## Repository layout
- `tdmpc2/` – TD-MPC2 agent, speculative execution, and corrector implementations plus utilities.
- `tdmpc2/scripts/` – entry points for data collection (`collect_corrector_data.py`) and evaluation (`eval_corrector.py`).
- `tdmpc2/tdmpc2/` – TD-MPC2 training (`train.py`), corrector training (`train_corrector.py`), configs, and helpers.
- `scripts/` – Decision Transformer training (`train.py`), evaluation (`test.py`), and plotting (`plot.py`).
- `data/` – location for D4RL datasets and saved TD-MPC2 / corrector buffers.
- `gym_qwen.yaml`, `cmd.sh` – example experiment configuration and shell helpers.

## Installation
1. Use Python 3.9+ and create a virtual environment or conda env.
2. Install Python dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. External system notes:
   - MuJoCo / `mujoco-py` is required for MuJoCo-based tasks.
   - Install D4RL for Decision Transformer baselines; datasets download automatically if you run the helper script below.

## Preparing data
- **Decision Transformer**: place D4RL datasets under `data/` (default) and run:
  ```bash
  python3 data/download_d4rl_datasets.py
  ```
- **TD-MPC2**: on-policy experience is generated by the TD-MPC2 training scripts; no offline dataset prep is required.

## Running TD-MPC2 experiments
Below are representative commands; adjust task names and paths as needed.

### Train baseline TD-MPC2 (replan every step)
```bash
python tdmpc2/tdmpc2/train.py task=dog-run model_size=5 device=cuda
```

### Collect corrector training data from a trained TD-MPC2 checkpoint
```bash
python tdmpc2/scripts/collect_corrector_data.py \
  --env dog-run \
  --checkpoint checkpoints/tdmpc2_dog_run.pt \
  --episodes 50 \
  --output data/corrector_data.pt \
  --device cuda
```
Use `--min_distance` to filter small mismatches and `--max_samples` to cap collection size.

### Train the corrector by distillation
Two-tower MLP corrector:
```bash
python tdmpc2/tdmpc2/train_corrector.py \
  --data data/corrector_data.pt \
  --tdmpc_ckpt checkpoints/tdmpc2_dog_run.pt \
  --corrector_type two_tower \
  --epochs 50 \
  --device cuda \
  --save_path checkpoints/corrector_two_tower.pt
```
Temporal transformer corrector (uses mismatch history):
```bash
python tdmpc2/tdmpc2/train_corrector.py \
  --data data/corrector_data.pt \
  --tdmpc_ckpt checkpoints/tdmpc2_dog_run.pt \
  --corrector_type temporal \
  --history_len 4 \
  --epochs 50 \
  --device cuda \
  --save_path checkpoints/corrector_temporal.pt
```

### Evaluate
- **Baseline TD-MPC2 (no speculation):**
  ```bash
  python tdmpc2/scripts/eval_corrector.py \
    --env dog-run \
    --tdmpc_checkpoint checkpoints/tdmpc2_dog_run.pt \
    --mode baseline \
    --episodes 10 \
    --device cuda
  ```
- **Speculative execution + corrector, 3-step horizon:**
  ```bash
  python tdmpc2/scripts/eval_corrector.py \
    --env dog-run \
    --tdmpc_checkpoint checkpoints/tdmpc2_dog_run.pt \
    --corrector_checkpoint checkpoints/corrector_two_tower.pt \
    --corrector_type two_tower \
    --mode spec_corrector \
    --spec_plan_horizon 3 \
    --spec_exec_horizon 3 \
    --episodes 10 \
    --device cuda
  ```
- **Speculative execution + corrector, extended 6-step horizon:**
  ```bash
  python tdmpc2/scripts/eval_corrector.py \
    --env dog-run \
    --tdmpc_checkpoint checkpoints/tdmpc2_dog_run.pt \
    --corrector_checkpoint checkpoints/corrector_two_tower.pt \
    --corrector_type two_tower \
    --mode spec6_corrector \
    --spec_plan_horizon 3 \
    --spec_exec_horizon 6 \
    --episodes 10 \
    --device cuda
  ```
Modes switch between baseline, naïve 3-step open-loop, speculative with correction, and speculative with extended execution.

## Decision Transformer baselines
Train on a D4RL dataset:
```bash
python3 scripts/train.py --env halfcheetah --dataset medium --device cuda
```
Evaluate a trained checkpoint:
```bash
python3 scripts/test.py --env halfcheetah --dataset medium --device cpu --num_eval_ep 1 --chk_pt_name <checkpoint.pt>
```
Plot logged curves:
```bash
python3 scripts/plot.py --env_d4rl_name halfcheetah-medium-v2 --smoothing_window 5 --plot_avg --save_fig
```

## Reproducibility tips
- Set seeds via CLI flags (e.g., `seed=` for TD-MPC2 Hydra configs or `--seed` flags in helper scripts).
- Default logs and checkpoints are written under the working directory (e.g., `runs/`, `dt_runs/`, or paths configured in `gym_qwen.yaml` and `tdmpc2/tdmpc2/config.yaml`).
- Keep configuration files (`gym_qwen.yaml`, `tdmpc2/tdmpc2/config.yaml`) under version control for consistent experiments.

## Citing and related work
This repository builds on TD-MPC and TD-MPC2 but is **not** an official implementation.
- TD-MPC code: https://github.com/nicklashansen/tdmpc
- TD-MPC paper: “Temporal Difference Learning for Model Predictive Control” (Hansen et al., 2022), arXiv:2203.04955
- TD-MPC2 code: https://github.com/nicklashansen/tdmpc2
- TD-MPC2 paper: “TD-MPC2: Scalable, Robust World Models for Continuous Control” (Hansen et al., 2023), arXiv:2310.16828
- Decision Transformer baseline from: https://github.com/nikhilbarhate99/min-decision-transformer

## License
This project is released under the MIT License; see `LICENSE` for details.
